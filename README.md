# PrevisÃ£o de InadimplÃªncia com Machine Learning
![image](https://github.com/user-attachments/assets/9517bf50-02e0-4870-92c9-526318479e13)



Este projeto tem como objetivo prever a inadimplÃªncia de clientes no momento da solicitaÃ§Ã£o de um cartÃ£o de crÃ©dito, utilizando tÃ©cnicas de Machine Learning aplicadas a dados cadastrais, comportamentais e socioeconÃ´micos. A soluÃ§Ã£o visa auxiliar uma fintech na tomada de decisÃµes mais seguras e eficazes na concessÃ£o de crÃ©dito.

---

## ğŸ“Œ Contexto

A anÃ¡lise de crÃ©dito Ã© essencial para fintechs que atuam em ambientes altamente competitivos e orientados por dados. Com o avanÃ§o da ciÃªncia de dados, modelos preditivos vÃªm sendo usados para reduzir perdas com inadimplÃªncia e promover inclusÃ£o financeira, possibilitando aprovaÃ§Ãµes mais justas e seguras.

---

## ğŸ¯ Objetivo do Projeto

Identificar, com base em dados histÃ³ricos e cadastrais, quais clientes tÃªm maior probabilidade de nÃ£o pagarem suas dÃ­vidas, permitindo que a fintech:

- Reduza riscos financeiros;
- Otimize limites e condiÃ§Ãµes de crÃ©dito;
- Tome decisÃµes mais precisas na aprovaÃ§Ã£o de clientes.

---

## ğŸ§© Bases de Dados Utilizadas

Foram utilizados trÃªs datasets principais, posteriormente unificados no dataframe `df_unique`:

- `application_train_dataset.csv`: Dados demogrÃ¡ficos, socioeconÃ´micos e comportamentais dos clientes.
- `customers_target_and_decision_dataset.csv`: ContÃ©m a variÃ¡vel alvo (`default`) e a decisÃ£o sobre o crÃ©dito.
- `unstructured_dataset.csv`: Dados nÃ£o estruturados que complementam o perfil do cliente.

---

## ğŸ› ï¸ PrÃ©-Processamento

As principais etapas foram:

1. **ConversÃ£o de datas e extraÃ§Ã£o de componentes temporais** (ano, mÃªs, dia da semana, etc.);
2. **CriaÃ§Ã£o da variÃ¡vel `days_diff`**: diferenÃ§a em dias entre a data de score e a data do pagamento;
3. **Tratamento de valores ausentes**:
   - `flag_document_A`: preenchido com a moda;
4. **TransformaÃ§Ãµes categÃ³ricas**:
   - `gender`: binarizado (`m=1`, `f=0`);
   - `flag_document_A`: convertido para inteiro;
5. **One-Hot Encoding**: aplicado a colunas categÃ³ricas com poucos nÃ­veis (ex: `ext_score_2`);
6. **Target Encoding com validaÃ§Ã£o cruzada**: aplicado Ã  variÃ¡vel `occupation_type`;
7. **RemoÃ§Ã£o de colunas irrelevantes**: `channel`, `ids`, `score_date`, `date`.

---

## âš™ï¸ Modelos Aplicados

### ğŸ”¹ Baseline
- Alto desempenho nas mÃ©tricas convencionais (acurÃ¡cia: 99%), mas AUC de apenas **0.489**.
- Fraca capacidade de separaÃ§Ã£o entre classes â†’ necessidade de modelos mais robustos.

### ğŸ”¹ RegressÃ£o LogÃ­stica
- AUC: **0.978**
- F1-score: 0.84 (classe 1)
- Recall da classe 1: 0.78
- **Boa separaÃ§Ã£o entre as classes**, mas perde alguns inadimplentes.

### ğŸ”¹ Random Forest
- AUC: **1.0000**
- AcurÃ¡cia: 100%
- Falsos positivos: 11 | Falsos negativos: 108
- Desempenho excelente, mas precisa de avaliaÃ§Ã£o quanto a **possÃ­vel overfitting**.

### ğŸ”¹ XGBoost
- AUC: **0.9933**
- F1-score: 0.94 (mÃ©dia macro)
- MAE (probabilidades): **0.0550**
- Baixo nÃºmero de falsos positivos e **alta confiabilidade nas previsÃµes**.

---

## ğŸ“Š ImportÃ¢ncia das VariÃ¡veis

Para todos os modelos foram gerados grÃ¡ficos de **feature importance**. As variÃ¡veis mais relevantes incluem:

- occupation_type
- flag_document_A
- start_hour
- score_weekofyear
- credit_card_initial_line
- payment

---

## ğŸ“ˆ AvaliaÃ§Ã£o de Performance

| Modelo            | AcurÃ¡cia | Recall (Classe 1) | F1-score (Classe 1) | AUC    |
|-------------------|----------|-------------------|----------------------|--------|
| Baseline          | 0.99     | 0.93              | 0.96                 | 0.489  |
| RegressÃ£o LogÃ­stica | 0.95   | 0.78              | 0.84                 | 0.978  |
| Random Forest     | 1.00     | 0.99              | 0.99                 | 1.000  |
| XGBoost           | 0.97     | 0.83              | 0.91                 | 0.9933 |

---


## ğŸ“ Estrutura do Projeto

A estrutura de diretÃ³rios do projeto foi organizada da seguinte forma:


```bash
credit_fintech/
â”œâ”€â”€ data/                    # Dados brutos e processados
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/               # Notebooks com anÃ¡lises e modelagens
â”œâ”€â”€ models/                  # Modelos treinados (.pkl)
â”œâ”€â”€ reports/                 # RelatÃ³rios e figuras
â”‚   â””â”€â”€ figures/
â”œâ”€â”€ src/                     # CÃ³digo-fonte do projeto
â”‚   â”œâ”€â”€ preprocessing.py     # Pipeline de prÃ©-processamento
â”‚   â””â”€â”€ train_models.py      # Treinamento dos modelos
â”œâ”€â”€ requirements.txt         # DependÃªncias do projeto
â”œâ”€â”€ README.md                # DocumentaÃ§Ã£o do projeto
â””â”€â”€ .gitignore
```

---

## âœ… ConclusÃµes

- O modelo Random Forest apresentou desempenho quase perfeito, mas precisa ser testado em produÃ§Ã£o para evitar overfitting.
- O XGBoost combinou alta performance e boa generalizaÃ§Ã£o, sendo uma excelente alternativa de produÃ§Ã£o.
- O modelo de RegressÃ£o LogÃ­stica pode servir como benchmark por sua interpretabilidade e desempenho sÃ³lido.
- MÃ©tricas como AUC, anÃ¡lise da distribuiÃ§Ã£o dos scores e anÃ¡lise de erro foram fundamentais para avaliar os modelos de forma profunda.

---

## ğŸ“ˆ PrÃ³ximos Passos

- Monitoramento do modelo em produÃ§Ã£o com foco em **drift detection**
- CalibraÃ§Ã£o de probabilidades
- Teste de modelos mais avanÃ§ados (e.g. reinforcement learning)
- InclusÃ£o de dados comportamentais em tempo real

---

## ğŸ§ª Tecnologias Utilizadas

- Python 3.11  
- Pandas, NumPy  
- Scikit-learn  
- XGBoost  
- Matplotlib, Seaborn  
- SHAP  
- Jupyter Notebook  

---

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas!  
Abra uma *issue* ou envie um *pull request* com sugestÃµes ou melhorias.

---

## ğŸ‘©â€ğŸ’» Autora

**Josiele Ferreira**  
*Cientista de Dados*  
ğŸ”— [LinkedIn](https://https://www.linkedin.com/in/josiele-ferreira-90686a1b2/)

---



